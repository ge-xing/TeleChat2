# MOE模型

### MOE模型介绍

TeleChat2-39B-A12B模型采用MoE架构，总32路由专家，激活8个专家，共39B参数量，实际激活参数为12B。

### 技术创新-训练方式

采用课程学习的方式，首先聚焦低难度、高质量教育知识以及多语言数据进行模型训练，以获得较好的模型初始性能；然后引入复杂数据，增大数学、逻辑推理、代码等数据占比，提升模型逻辑推理能力；最后，使用高质量数据进行退火，持续提升模型效果；
### 技术创新-国产算力优化

在MOE模块将Tensor并行域转换成专家并行域，从而将MOE的AllToAll 通讯约束在节点内，提高通讯效率;把MOE输入切成多个副本依次下发，将dispatch通信/FFN计算/combine通信三个环节连成流水线，实现moe的计算通信掩盖;基于对内存和计算的开销建模，理论求解内存约束下性能最优的流水线并行的负载配置，实现流水线负载均衡。
### 效果评测

综合评测数据集上，TeleChat2-39B-A12B模型以12B激活参数量接近TeleChat2-35B模型效果。

| Dataset    | TeleChat2-35B | TeleChat2-39B-A12B | TeleChat2-7B | TeleChat2-3B |
| ---------- | ------------- | ------------------ | ------------ | ------------ |
| C-Eval     | 85            | 89                 | 82           | 75           |
| MMLU       | 82            | 83                 | 79.6         | 72.9         |
| CMMLU      | 90.18         | 90                 | 84.6         | 73           |
| GSM8K      | 91            | 83.5               | 86.8         | 64.7         |
| HumanEval  | 73            | 68                 | 56           | 38           |
| MBPP       | 75            | 67                 | 62.6         | 47           |
| AlignBench | 7.88          | 7.56               | 6.96         | 5.74         |
| IFEval     | 79.63         | 76.48              | 73.1         | 61.29        |
